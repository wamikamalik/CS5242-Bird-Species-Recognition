{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXX0qlqf0D0C",
        "outputId": "25b6394b-d03f-44b9-9039-dccb0b925415"
      },
      "outputs": [],
      "source": [
        "# For Google Colaboratory\n",
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "\n",
        "    # mount google drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    path_to_file = '/content/gdrive/My Drive/CS5242/Project/' # Update this path depending on where data is uploaded on your Google Drive.\n",
        "    print(path_to_file)\n",
        "    # move to Google Drive directory\n",
        "    os.chdir(path_to_file)\n",
        "    !pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unzip files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZWa7uKl1ZjH",
        "outputId": "451f179f-00c7-44e3-e817-38cc1e4c2bcc"
      },
      "outputs": [],
      "source": [
        "!ls\n",
        "\n",
        "# !tar -xzf \"/content/gdrive/My Drive/CS5242/Project/CUB_200_2011.tgz\" -C \"/content/gdrive/My Drive/CS5242/Project/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge all Metadata "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "gQISrpSsS8yN",
        "outputId": "05e88dca-5dbd-4493-b8fd-4d8af0f930f4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "images_file = './CUB_200_2011/images.txt'\n",
        "train_test_split_file = './CUB_200_2011/train_test_split.txt'\n",
        "classes_file = './CUB_200_2011/classes.txt'\n",
        "image_class_labels_file = './CUB_200_2011/image_class_labels.txt'\n",
        "bounding_boxes = './CUB_200_2011/bounding_boxes.txt'\n",
        "\n",
        "# Load each file into a DataFrame\n",
        "df_images = pd.read_csv(images_file, sep=\" \", names=['image_id', 'image_name'], header=None)\n",
        "df_train_test_split = pd.read_csv(train_test_split_file, sep=\" \", names=['image_id', 'is_training_image'], header=None)\n",
        "df_classes = pd.read_csv(classes_file, sep=\" \", names=['class_id', 'class_name'], header=None)\n",
        "df_image_class_labels = pd.read_csv(image_class_labels_file, sep=\" \", names=['image_id', 'class_id'], header=None)\n",
        "df_bound = pd.read_csv(bounding_boxes, sep=\" \", names=['image_id', 'bounding_x','bounding_y','bounding_width','bounding_height'], header=None)\n",
        "\n",
        "# Merge the DataFrames\n",
        "df_merged = pd.merge(df_images, df_image_class_labels, on='image_id')\n",
        "df_merged = pd.merge(df_merged, df_classes, on='class_id')\n",
        "df_merged = pd.merge(df_merged, df_train_test_split, on='image_id')\n",
        "df_merged = pd.merge(df_merged, df_bound, on='image_id')\n",
        "\n",
        "# Merge bounding boxes\n",
        "\n",
        "display(df_merged.sample(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "fArbZtweNbU7",
        "outputId": "b02d9299-1741-487e-c5c2-7a1e34801790"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "def get_shape(x):\n",
        "  image_filename = x['image_name']\n",
        "  image_path = f'./CUB_200_2011/images/{image_filename}'\n",
        "  img = Image.open(image_path)\n",
        "  width, height = img.size\n",
        "  x['width'] = width\n",
        "  x['height'] = height\n",
        "  return x\n",
        "\n",
        "df_merged = df_merged.apply(lambda x: get_shape(x), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhAWtnSgRiHa"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "points = ax.scatter(df_merged.width, df_merged.height, color='blue', alpha=0.5, picker=True)\n",
        "ax.set_title(\"Image Resolution\")\n",
        "ax.set_xlabel(\"Width\", size=14)\n",
        "ax.set_ylabel(\"Height\", size=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tras6YmBLL4j"
      },
      "source": [
        "- Training: 5994 (Classes are quite balanced, only 6 classes with 29, the rest 30)\n",
        "- Testing: 5749\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJoZpVnfmDeX"
      },
      "outputs": [],
      "source": [
        "display(df_merged['class_name'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb32Zm-AaSE2"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows',None)\n",
        "display(df_merged['is_training_image'].value_counts())\n",
        "print(df_merged[df_merged['is_training_image'] == 1].class_name.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD0qsWeeWwTk"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "sample_image = df_merged.sample(1)\n",
        "sample_image_name = sample_image['image_name'].iloc[0]\n",
        "\n",
        "# Accessing individual bounding box attributes\n",
        "bbox_x = sample_image['bounding_x'].values[0]\n",
        "bbox_y = sample_image['bounding_y'].values[0]\n",
        "bbox_width = sample_image['bounding_width'].values[0]\n",
        "bbox_height = sample_image['bounding_height'].values[0]\n",
        "\n",
        "image = cv2.imread(f'./CUB_200_2011/images/{sample_image_name}')\n",
        "plt.imshow(image)\n",
        "print(image.shape)\n",
        "rect = patches.Rectangle((bbox_x, bbox_y), bbox_width, bbox_height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "\n",
        "# Add the rect to the Axes\n",
        "plt.gca().add_patch(rect)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u58szo6suoAQ"
      },
      "source": [
        "### Data augmentation\n",
        "- After train-test split (Prevent leakage)\n",
        "- Done on training set only, test set only resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "ivpiRXgRuX7u",
        "outputId": "df620223-ed66-4b10-e6bb-9eec659c7543"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import Lambda\n",
        "\n",
        "# Custom transform to convert images to RGB\n",
        "to_rgb = Lambda(lambda x: x.convert(\"RGB\"))\n",
        "\n",
        "# Define your transformations\n",
        "transform = transforms.Compose([\n",
        "    to_rgb,\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=45),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #Common practice to use standard normalize value from ImageNet\n",
        "])\n",
        "\n",
        "sample_image = df_merged.sample(1)\n",
        "sample_image_name = sample_image['image_name'].iloc[0]\n",
        "\n",
        "# Accessing individual bounding box attributes\n",
        "bbox_x = sample_image['bounding_x'].values[0]\n",
        "bbox_y = sample_image['bounding_y'].values[0]\n",
        "bbox_width = sample_image['bounding_width'].values[0]\n",
        "bbox_height = sample_image['bounding_height'].values[0]\n",
        "\n",
        "# Load an image\n",
        "image_path = f'./CUB_200_2011/images/{sample_image_name}'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Apply the transformations\n",
        "transformed_image = transform(image)\n",
        "\n",
        "# Convert the transformed tensor to a NumPy array and transpose the dimensions\n",
        "# from (C, H, W) to (H, W, C) for visualization\n",
        "transformed_image = transformed_image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "cropped_image = image.crop((bbox_x, bbox_y, bbox_x+bbox_width, bbox_y+bbox_height))\n",
        "transformed_cropped = transform(cropped_image).numpy().transpose((1, 2, 0))\n",
        "# Setup the subplot\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "# Display the original image\n",
        "axs[0].imshow(image)\n",
        "axs[0].set_title('Original Image')\n",
        "axs[0].axis('off')  # Remove axis ticks and labels\n",
        "\n",
        "# Display the transformed image\n",
        "axs[1].imshow(transformed_image,aspect='auto')\n",
        "axs[1].set_title('Transformed Image')\n",
        "axs[1].axis('off')  # Remove axis ticks and labels\n",
        "\n",
        "# Display the cropped image\n",
        "axs[2].imshow(cropped_image,aspect='auto')\n",
        "axs[2].set_title('Cropped Image')\n",
        "axs[2].axis('off')  # Remove axis ticks and labels\n",
        "\n",
        "# Display the transformed cropped image\n",
        "axs[3].imshow(transformed_cropped,aspect='auto')\n",
        "axs[3].set_title('Transformed Cropped Image')\n",
        "axs[3].axis('off')  # Remove axis ticks and labels\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1GlsqVsWmEc"
      },
      "source": [
        "### Finalised Preprocessing Steps\n",
        "1. Resize to (224,224)\n",
        "2. Training_set transformation: random horizontal flip and rotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ExiSjwsW19e",
        "outputId": "ac6f4329-84d3-4d71-80eb-37467d22d4aa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm  # for progress bar\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "# Assuming df_merged is your DataFrame\n",
        "# Ensure the output directory exists\n",
        "output_dir = './augmented_images/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Function to apply transformations including cropping to the bounding box\n",
        "def apply_transforms(image, bbox, is_training):\n",
        "    # Crop the image first\n",
        "    image = F.crop(image, bbox['y'], bbox['x'], bbox['height'], bbox['width'])\n",
        "\n",
        "    # Apply training or test transformations\n",
        "    if is_training:\n",
        "        image = train_transform(image)\n",
        "    else:\n",
        "        image = test_transform(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Define transformations for training and test sets, without Resize here, since cropping is first\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=45),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Update column for augmented image names\n",
        "df_merged['augmented_image_name'] = None\n",
        "\n",
        "for index, row in tqdm(df_merged.iterrows(), total=df_merged.shape[0]):\n",
        "    image_path = f'./CUB_200_2011/images/{row[\"image_name\"]}'\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Accessing individual bounding box attributes directly from the row\n",
        "    bbox = {\n",
        "        'x': row['bounding_x'],\n",
        "        'y': row['bounding_y'],\n",
        "        'width': row['bounding_width'],\n",
        "        'height': row['bounding_height']\n",
        "    }\n",
        "\n",
        "    # Apply transformations including cropping\n",
        "    transformed_image = apply_transforms(image, bbox, row['is_training_image'])\n",
        "\n",
        "    # Convert the transformed tensor to PIL Image to save it (if not already a PIL Image)\n",
        "    if not isinstance(transformed_image, Image.Image):\n",
        "        transformed_image = F.to_pil_image(transformed_image)\n",
        "\n",
        "    # Save augmented image\n",
        "    augmented_image_path = os.path.join(output_dir, row[\"image_name\"])\n",
        "    os.makedirs(os.path.dirname(augmented_image_path), exist_ok=True)\n",
        "    transformed_image.save(augmented_image_path)\n",
        "\n",
        "    # Update DataFrame with new image path\n",
        "    df_merged.at[index, 'augmented_image_name'] = augmented_image_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "qQsmxMSukaT_",
        "outputId": "225b03db-f29c-4781-952b-e81da9e8fbfc"
      },
      "outputs": [],
      "source": [
        "display(df_merged)\n",
        "df_merged.to_csv('./meta_data.csv', index=False)\n",
        "from google.colab import files\n",
        "files.download('meta_data.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
